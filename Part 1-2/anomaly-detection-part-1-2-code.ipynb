{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0735a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## ANOMALY DETECTION PART 1 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 27: Loading packages  ####\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08139ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 28: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 29: Load the dataset  ####\n",
    "\n",
    "paysim = pd.read_csv(str(data_dir)+\"/paysim_transactions.csv\")\n",
    "paysim.head()\n",
    "paysim.info()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca73fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 30: Understand the dataset  ####\n",
    "\n",
    "paysim.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 31: Target of the dataset  ####\n",
    "\n",
    "paysim['isFraud'].value_counts()\n",
    "\n",
    "print(paysim['isFraud'].value_counts() / len(paysim))\n",
    "paysim['isFraud'].value_counts().plot(kind = 'bar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 32: EDA of the dataset  ####\n",
    "\n",
    "paysim.isnull().sum()\n",
    "paysim['type'].value_counts().plot(kind = 'bar')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 34: Subset the data  ####\n",
    "\n",
    "# Subset variables from fraud dataset\n",
    "paysim_dbscan = paysim.drop(['step', 'type','nameOrig', 'nameDest', 'isFlaggedFraud','isFraud'], axis = 1)\n",
    "print(paysim_dbscan.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432deddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 35: Data cleaning: NAs  ####\n",
    "\n",
    "print(paysim_dbscan.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b593a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 36: StandardScalar  ####\n",
    "\n",
    "# Instantiate MinMaxScaler.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the dataframe.\n",
    "paysim_dbscan_scaled = scaler.fit_transform(paysim_dbscan)\n",
    "# Convert back to dataframe, making sure to name the columns again.\n",
    "paysim_dbscan_scaled = pd.DataFrame(paysim_dbscan_scaled, columns = paysim_dbscan.columns)\n",
    "print(paysim_dbscan_scaled.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b94146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: DBSCAN: model   ####\n",
    "\n",
    "# Let's run DBSCAN.\n",
    "dbscan = DBSCAN(eps=0.2, min_samples = 5)\n",
    "clusters = dbscan.fit_predict(paysim_dbscan_scaled)\n",
    "\n",
    "# Check the number of clusters\n",
    "unique, counts = np.unique(clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1a9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 39: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 44: Optimal Eps determination  ####\n",
    "\n",
    "nn_model = NearestNeighbors(n_neighbors=10)\n",
    "nbrs = nn_model.fit(paysim_dbscan_scaled)\n",
    "distances, indices = nbrs.kneighbors(paysim_dbscan_scaled)\n",
    "distances = np.mean(distances,axis=1)\n",
    "distances = np.sort(distances, axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 47: DBSCAN: optimized model  ####\n",
    "\n",
    "# DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples = 10)\n",
    "optimized_clusters = dbscan.fit_predict(paysim_dbscan_scaled)\n",
    "\n",
    "# Check the number of clusters\n",
    "unique, counts = np.unique(optimized_clusters, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "paysim_dbscan['cluster'] = optimized_clusters\n",
    "\n",
    "paysim_dbscan.loc[paysim_dbscan['cluster'] >= 0,'cluster'] = 0\n",
    "paysim_dbscan.loc[paysim_dbscan['cluster'] == -1,'cluster'] = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 48: Visualize the Anomalies  ####\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(paysim_dbscan['oldbalanceOrg'], paysim_dbscan['newbalanceOrig'], c = paysim_dbscan['cluster'], alpha = 0.8)\n",
    "ax.legend(*sc.legend_elements())\n",
    "plt.xlabel('Amount of transaction')\n",
    "plt.ylabel('Origin new balance after transaction')\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(paysim_dbscan['oldbalanceOrg'], paysim_dbscan['newbalanceOrig'], c = paysim_dbscan['cluster'], alpha = 0.8)\n",
    "ax.legend(*sc.legend_elements())\n",
    "plt.xlabel('Amount of transaction')\n",
    "plt.ylabel('Origin new balance after transaction')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d2610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 49: Visualize the Anomalies  ####\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(paysim_dbscan['amount'], paysim_dbscan['newbalanceDest'], c = paysim_dbscan['cluster'], alpha = 0.8)\n",
    "ax.legend(*sc.legend_elements())\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Recipient new Balance after transaction')\n",
    "plt.show()\n",
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(paysim_dbscan['amount'], paysim_dbscan['newbalanceDest'], c = paysim_dbscan['cluster'], alpha = 0.8)\n",
    "ax.legend(*sc.legend_elements())\n",
    "plt.xlabel('Amount')\n",
    "plt.ylabel('Recipient new Balance after transaction')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92220e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 52: Exercise 2  ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## ANOMALY DETECTION PART 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b79551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 4: Loading packages  ####\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "!pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 5: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path \n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent\n",
    "print(main_dir)\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "print(data_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcb78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 6: Time series data: load energy consumption  ####\n",
    "\n",
    "pjm_energy = pd.read_csv(str(data_dir)+\"/PJME_hourly.csv\")\n",
    "pjm_energy.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Time series data: preprocessing  ####\n",
    "\n",
    "pjm_energy['Datetime'] = pd.to_datetime(pjm_energy['Datetime'])\n",
    "pjm_energy.info()\n",
    "pjm_energy = pjm_energy[pjm_energy['Datetime'] > '2018-01-01 00:00:00']\n",
    "pjm_energy.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eeb4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 8: Visualize the data: line plot  ####\n",
    "\n",
    "pjm_energy.plot(x='Datetime', y='PJME_MW', figsize=(17,5))\n",
    "plt.xlabel('Date time')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Energy consumption (MW) at each hour in 2018')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b83884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 9: Scaling on time series data  ####\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the dataframe.\n",
    "pjm_energy_scaled = scaler.fit_transform(pd.DataFrame(pjm_energy['PJME_MW']))\n",
    "print(pjm_energy_scaled)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75c76b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 12: DBSCAN on time series data  ####\n",
    "\n",
    "dbscan_energy = DBSCAN(eps = 0.03, metric='euclidean', min_samples=5, n_jobs = -1)\n",
    "\n",
    "pjm_energy['anomaly'] =  dbscan_energy.fit_predict(pd.DataFrame(pjm_energy_scaled))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e16efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 14: Anomalies detected  ####\n",
    "\n",
    "pjm_energy[pjm_energy['anomaly'] == -1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529cab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 16: Exercise 3  ####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 29: Prepare a dataset for decision tree modeling  ####\n",
    "\n",
    "paysim = pd.read_csv(str(data_dir)+\"/paysim_transactions.csv\")\n",
    "# Drop columns. \n",
    "paysim = paysim.drop(['step', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis = 1)\n",
    "paysim.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2097b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 30: Convert categorical to dummy  ####\n",
    "\n",
    "paysim['type'] = pd.Categorical(paysim['type'])\n",
    "paysim['type'] = paysim['type'].cat.codes\n",
    "colname = pd.get_dummies(paysim['type'], prefix = 'type', drop_first = True)\n",
    "paysim = pd.concat([paysim, colname], axis = 1)\n",
    "paysim.drop(['type'], axis = 1, inplace = True)\n",
    "\n",
    "paysim.columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cd6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 31: Decision tree classification  ####\n",
    "\n",
    "# Select predictors and target.\n",
    "y = paysim['isFraud']\n",
    "X = paysim.drop(['isFraud'], axis = 1)\n",
    "\n",
    "# Build a logistic regression model.\n",
    "np.random.seed(1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30)\n",
    "dtree = DecisionTreeClassifier(max_depth = 10)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 32: Predict the target  ####\n",
    "\n",
    "dtree_y_train_pred = dtree.predict(X_train)\n",
    "dtree_y_test_pred = dtree.predict(X_test)\n",
    "dtree_accuracy = metrics.accuracy_score(y_test, dtree_y_test_pred)\n",
    "print(\"Accuracy of test data:\\t\", dtree_accuracy)\n",
    "# ROC AUC value.\n",
    "roc_auc_score(y_test, dtree_y_test_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa496f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 33: Confusion matrix of training data  ####\n",
    "\n",
    "print('Confusion Matrix - Training Dataset')\n",
    "print(pd.crosstab(y_train, dtree_y_train_pred, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print('Percentage of accurate fraud cases is ', 133/153)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c712198",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 34: Confusion matrix of test data  ####\n",
    "\n",
    "print('Confusion Matrix - Testing Dataset')\n",
    "print(pd.crosstab(y_test, dtree_y_test_pred, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print('Percentage of accurate fraud cases is', 45/67)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abd7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 37: Find TPR and TNR  ####\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, dtree_y_test_pred).ravel()\n",
    "\n",
    "# Find the TNR.\n",
    "non_fraud_eval = tn / (tn + fp)\n",
    "print(non_fraud_eval)\n",
    "# Find the TPR.\n",
    "fraud_eval = tp / (tp + fn)\n",
    "print(fraud_eval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: Save the metric  ####\n",
    "\n",
    "performance_df = pd.DataFrame(columns = ['model_name', 'TPR', 'TNR'])\n",
    "\n",
    "s = pd.Series(['Decision_tree_baseline', fraud_eval, non_fraud_eval], \n",
    "              index=['model_name', 'TPR', 'TNR'])\n",
    "performance_df = performance_df.append(s, ignore_index = True)\n",
    "performance_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd6962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 40: Exercise 4  ####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0eb86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 49: SMOTE in fraud dataset  ####\n",
    "\n",
    "sm = SMOTE(random_state = 1)\n",
    "X_train_new, y_train_new = sm.fit_resample(X_train, y_train)\n",
    "# Shape of X_train.\n",
    "print(X_train.shape)\n",
    "# Print shape of X_train_new.\n",
    "print(X_train_new.shape)\n",
    "# Double check that the data has been balanced.\n",
    "pd.Series(y_train_new).value_counts().plot.bar()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d1d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 50: Fit the model and predict  ####\n",
    "\n",
    "# Fit the model.\n",
    "dtree.fit(X_train_new, y_train_new)\n",
    "\n",
    "# Prediction for training data.\n",
    "train_pred_sm = dtree.predict(X_train_new)\n",
    "\n",
    "# Prediction for the test data.\n",
    "test_pred_sm = dtree.predict(X_test)\n",
    "train_pred_sm = dtree.predict(X_train_new)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 51: Confusion matrix of training data  ####\n",
    "\n",
    "print('Confusion Matrix - Training Dataset')\n",
    "print(pd.crosstab(y_train_new, train_pred_sm, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print('Percentage of accurate fraud cases: ', 13999/14001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 52: Confusion matrix of test data  ####\n",
    "\n",
    "print('Confusion Matrix - Testing Dataset')\n",
    "print(pd.crosstab(y_test, test_pred_sm, rownames = ['True'], colnames = ['Predicted'], margins = True))\n",
    "print('Percentage of accurate fraud cases ', 58/67)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b145556",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 53: Find TPR and TNR and save  ####\n",
    "\n",
    "# Find TPR and TNR and save the result.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, test_pred_sm).ravel()\n",
    "non_fraud_eval = tn / (tn + fp)\n",
    "print(non_fraud_eval)\n",
    "fraud_eval = tp / (tp + fn)\n",
    "print(fraud_eval)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 54: Add scores to the performance dataframe  ####\n",
    "\n",
    "s = pd.Series(['SMOTE', fraud_eval, non_fraud_eval], \n",
    "              index=['model_name', 'TPR', 'TNR'])\n",
    "performance_df = performance_df.append(s, ignore_index = True)\n",
    "performance_df\n",
    "pickle.dump(performance_df, open(str(data_dir) + \"/performance_anomalies.sav\",\"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 56: Exercise 5  ####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################################\n",
    "####  CONGRATULATIONS ON COMPLETING THIS MODULE!   ####\n",
    "#######################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
